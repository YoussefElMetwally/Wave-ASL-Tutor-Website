{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a51310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_asl_corrected.py\n",
    "# This script performs real-time sign language recognition using a pre-trained model.\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "from collections import deque\n",
    "\n",
    "# --- 1. Global Constants and Helper Functions ---\n",
    "\n",
    "# --- Landmark & Preprocessing Constants (MUST MATCH TRAINING SCRIPT) ---\n",
    "NUM_POSE_LANDMARKS = 33\n",
    "NUM_HAND_LANDMARKS = 21\n",
    "POSE_LANDMARK_PAIRS = {11: 12, 13: 14, 15: 16, 23: 24, 25: 26, 27: 28, 29: 30, 31: 32}\n",
    "POSE_LANDMARK_PAIRS.update({v: k for k, v in POSE_LANDMARK_PAIRS.items()})\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"Processes an image with the MediaPipe Holistic model.\"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    \"\"\"Draws styled landmarks on the image.\"\"\"\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    if results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2))\n",
    "    if results.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2))\n",
    "    if results.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "def extract_keypoints_no_z(results):\n",
    "    \"\"\"\n",
    "    CORRECTED: Extracts keypoints WITHOUT Z for a vector length of 183.\n",
    "    This function is the main fix for the dimension mismatch error.\n",
    "    \"\"\"\n",
    "    pose = np.array([[res.x, res.y, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(NUM_POSE_LANDMARKS * 3)\n",
    "    lh = np.array([[res.x, res.y] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(NUM_HAND_LANDMARKS * 2)\n",
    "    rh = np.array([[res.x, res.y] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(NUM_HAND_LANDMARKS * 2)\n",
    "    return np.concatenate([pose, lh, rh])\n",
    "\n",
    "def mirror_keypoints_frame_no_z(keypoints_frame):\n",
    "    \"\"\"CORRECTED: Mirrors a 183-length keypoint vector.\"\"\"\n",
    "    mirrored_frame = np.copy(keypoints_frame)\n",
    "    # Mirror X-coords\n",
    "    for i in range(NUM_POSE_LANDMARKS): mirrored_frame[i * 3] = 1.0 - mirrored_frame[i * 3]\n",
    "    lh_start_idx = NUM_POSE_LANDMARKS * 3\n",
    "    rh_start_idx = lh_start_idx + NUM_HAND_LANDMARKS * 2\n",
    "    for i in range(NUM_HAND_LANDMARKS):\n",
    "        mirrored_frame[lh_start_idx + i * 2] = 1.0 - mirrored_frame[lh_start_idx + i * 2]\n",
    "        mirrored_frame[rh_start_idx + i * 2] = 1.0 - mirrored_frame[rh_start_idx + i * 2]\n",
    "    # Swap hands\n",
    "    lh_data = mirrored_frame[lh_start_idx:rh_start_idx].copy()\n",
    "    rh_data = mirrored_frame[rh_start_idx:].copy()\n",
    "    mirrored_frame[lh_start_idx:rh_start_idx] = rh_data\n",
    "    mirrored_frame[rh_start_idx:] = lh_data\n",
    "    # Swap pose\n",
    "    pose_data = mirrored_frame[:lh_start_idx].reshape(NUM_POSE_LANDMARKS, 3)\n",
    "    temp_pose_data = pose_data.copy()\n",
    "    for l_idx, r_idx in POSE_LANDMARK_PAIRS.items(): pose_data[l_idx] = temp_pose_data[r_idx]\n",
    "    mirrored_frame[:lh_start_idx] = pose_data.flatten()\n",
    "    return mirrored_frame\n",
    "\n",
    "def normalize_sequence_no_z(sequence_data):\n",
    "    \"\"\"CORRECTED: Robustly normalizes a sequence of 183-length keypoint vectors.\"\"\"\n",
    "    normalized_sequence = []\n",
    "    for frame_kps in sequence_data:\n",
    "        if np.all(frame_kps == 0):\n",
    "            normalized_sequence.append(frame_kps)\n",
    "            continue\n",
    "        \n",
    "        pose = frame_kps[:99].reshape(33, 3); lh = frame_kps[99:141].reshape(21, 2); rh = frame_kps[141:].reshape(21, 2)\n",
    "        \n",
    "        hip_l, hip_r = pose[23], pose[24]\n",
    "        origin = (hip_l[:2] + hip_r[:2]) / 2.0 if hip_l[2] > 0.5 and hip_r[2] > 0.5 else pose[0][:2]\n",
    "        pose[:, :2] -= origin; lh -= origin; rh -= origin\n",
    "        \n",
    "        shoulder_l, shoulder_r = pose[11], pose[12]\n",
    "        scale = np.linalg.norm(shoulder_l[:2] - shoulder_r[:2]) if shoulder_l[2] > 0.5 and shoulder_r[2] > 0.5 else 1.0\n",
    "        if scale < 1e-6: scale = 1.0\n",
    "        pose[:, :2] /= scale; lh /= scale; rh /= scale\n",
    "        \n",
    "        normalized_sequence.append(np.concatenate([pose.flatten(), lh.flatten(), rh.flatten()]))\n",
    "    return np.array(normalized_sequence)\n",
    "\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    \"\"\"Visualize prediction probabilities as a bar chart.\"\"\"\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0, 60 + num * 40), (int(prob * 100), 90 + num * 40), colors[num % len(colors)], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85 + num * 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return output_frame\n",
    "\n",
    "\n",
    "def main():\n",
    "    # --- 2. Configuration and Setup ---\n",
    "    SEQUENCE_LENGTH = 30\n",
    "    MODEL_PATH = os.path.join('models', 'best_action_model.h5')\n",
    "    LABEL_MAP_PATH = os.path.join('models', 'label_map.json')\n",
    "\n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"Error: Model file not found at {MODEL_PATH}\")\n",
    "        return\n",
    "    model = load_model(MODEL_PATH)\n",
    "\n",
    "    print(f\"Loading label map from: {LABEL_MAP_PATH}\")\n",
    "    if not os.path.exists(LABEL_MAP_PATH):\n",
    "        print(f\"Error: Label map file not found at {LABEL_MAP_PATH}\")\n",
    "        return\n",
    "    with open(LABEL_MAP_PATH, 'r') as f:\n",
    "        label_map = json.load(f)\n",
    "\n",
    "    # Create a reverse mapping from index to label string\n",
    "    actions = {int(v): k for k, v in label_map.items()}\n",
    "    print(f\"Model loaded. Actions: {actions}\")\n",
    "\n",
    "    # --- 3. Real-time Prediction Loop ---\n",
    "    sequence = deque(maxlen=SEQUENCE_LENGTH)\n",
    "    sentence = []\n",
    "    predictions = deque(maxlen=10) # For debouncing\n",
    "    threshold = 0.95\n",
    "    dominant_hand = 'RIGHT'\n",
    "\n",
    "    colors = [(245, 117, 16), (117, 245, 16), (16, 117, 245), (200, 100, 50), (50, 200, 100)]\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Ignoring empty camera frame.\")\n",
    "                continue\n",
    "\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            draw_styled_landmarks(image, results)\n",
    "            \n",
    "            # --- Prediction Logic ---\n",
    "            left_hand_detected = results.left_hand_landmarks is not None\n",
    "            right_hand_detected = results.right_hand_landmarks is not None\n",
    "            if left_hand_detected and not right_hand_detected:\n",
    "                dominant_hand = 'LEFT'\n",
    "            elif right_hand_detected:\n",
    "                dominant_hand = 'RIGHT'\n",
    "\n",
    "            keypoints = extract_keypoints_no_z(results)\n",
    "            if dominant_hand == 'LEFT':\n",
    "                keypoints = mirror_keypoints_frame_no_z(keypoints)\n",
    "            sequence.append(keypoints)\n",
    "\n",
    "            if len(sequence) == SEQUENCE_LENGTH:\n",
    "                # Preprocess the sequence exactly as in training\n",
    "                normalized_data = normalize_sequence_no_z(np.array(list(sequence)))\n",
    "                \n",
    "                # Make prediction\n",
    "                res = model.predict(np.expand_dims(normalized_data, axis=0))[0]\n",
    "                \n",
    "                prediction_idx = np.argmax(res)\n",
    "                predictions.append(prediction_idx)\n",
    "                \n",
    "                # Debouncing logic: ensure prediction is stable\n",
    "                if np.unique(list(predictions))[-1] == prediction_idx and len(predictions) == predictions.maxlen:\n",
    "                    if res[prediction_idx] > threshold:\n",
    "                        current_action = actions[prediction_idx]\n",
    "                        if not sentence or sentence[-1] != current_action:\n",
    "                            sentence.append(current_action)\n",
    "                \n",
    "                if len(sentence) > 5:\n",
    "                    sentence = sentence[-5:]\n",
    "                \n",
    "                # Visualize probabilities\n",
    "                top_indices = np.argsort(res)[-5:][::-1]\n",
    "                top_actions = [actions.get(i, 'N/A') for i in top_indices]\n",
    "                top_probs = res[top_indices]\n",
    "                image = prob_viz(top_probs, top_actions, image, colors)\n",
    "            \n",
    "            # --- Display UI Elements ---\n",
    "            cv2.rectangle(image, (0, 0), (280, 40), (245, 117, 16), -1)\n",
    "            cv2.putText(image, f'HAND: {dominant_hand}', (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.rectangle(image, (0, image.shape[0] - 40), (image.shape[1], image.shape[0]), (0, 0, 0), -1)\n",
    "            cv2.putText(image, ' '.join(sentence), (10, image.shape[0] - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.imshow('ASL Tutor - Real-time Prediction', image)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
